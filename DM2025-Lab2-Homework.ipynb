{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Eli≈°ka Kopeck√° (Eliska Kopecka)\n",
    "\n",
    "Student ID: 91499149X\n",
    "\n",
    "GitHub ID: username: elakop; numerical ID: 183389250\n",
    "\n",
    "Kaggle name: elakop (Eli≈°ka Kopeck√°) <- I can see both there, I'm not sure which one you require; with email elis.kop.b@gmail.com\n",
    "\n",
    "Kaggle private scoreboard snapshot: I took the picture  1.12.2025 at 23:22 (the end of the competition)\n",
    "\n",
    "![ranking](Screenshot_2025-12-01_232123.png)\n",
    "\n",
    "Today, the score changed. There were some late submissions (screenshot from 3.12.2025):\n",
    "\n",
    "![ranking2](Screenshot2025-12-03220153.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files and paths\n",
    "\n",
    "**Note:** I was orriginally runnig the code without jupiter notebook and these are the files I used\n",
    "\n",
    "I'm not sure if you will run my code, if yes I'll try to post the source files to github LAB2-EXCERCISE,\n",
    "but you'll need to change the DATA_DIR variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the file that I use for this lab\n",
    "# Maybe you will need to chnage the source of the data\n",
    "DATA_DIR = \"C:/Users/Administrator/Documents/MUNI/2/NTNU_fall/Data Mining/DM2025Labs/DM2025-Lab2-Exercise\"\n",
    "\n",
    "JSON_FILE = f\"{DATA_DIR}/final_posts.json\"\n",
    "EMOTION_FILE = f\"{DATA_DIR}/emotion.csv\"\n",
    "SPLIT_FILE = f\"{DATA_DIR}/data_identification.csv\"\n",
    "OUTPUT_FILE = f\"{DATA_DIR}/submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Preprocessing steps\n",
    "\n",
    "This section:\n",
    "- loads the data files\n",
    "- Merge them into a single DataFrame\n",
    "- Clean and preprocess the text data\n",
    "\n",
    "There are three data sources `final_posts.json`, which contains the actual text of posts (JSON structure), `emotion.csv`, which contains emotion labels for training data, and `data_identification.csv` with identifies which posts are for training vs testing\n",
    "\n",
    "With text files, I usually use encoding=utf-8 <- it actually happend to me, that I needed to add it to one of the Labs before.\n",
    "The JSON file has nested structure, actual data is buried inside `root` ‚Üí `_source` ‚Üí `post`. I had to look at the file first to figure out how to get to the `post_id` and `text` fields. That's why I'm using `item['root']['_source']['post']['post_id']` instead of just `item['post_id']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64171 posts from JSON\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text\n",
       "0  0x61fc95  We got the ranch, loaded our guns and sat up t...\n",
       "1  0x35663e  I bet there is an army of married couples who ...\n",
       "2  0xc78afe                         This could only end badly.\n",
       "3  0x90089c  My sister squeezed a lime in her milk when she...\n",
       "4  0xaba820         and that got my head bobbing a little bit."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load JSON file with posts\n",
    "with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "    posts_data = json.load(f)\n",
    "\n",
    "# Extract post_id and text from nested JSON structure\n",
    "posts = []\n",
    "for item in posts_data:\n",
    "    post_id = item['root']['_source']['post']['post_id']\n",
    "    text = item['root']['_source']['post']['text']\n",
    "    posts.append({'id': post_id, 'text': text})\n",
    "\n",
    "posts_df = pd.DataFrame(posts)\n",
    "print(f\"{len(posts_df)} posts from JSON\")\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the CSV with emotion labels (anger, joy, fear, etc.) for each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47890 emotion labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id emotion\n",
       "0  0x35663e     joy\n",
       "1  0xc78afe    fear\n",
       "2  0x90089c     joy\n",
       "3  0x2ffb63     joy\n",
       "4  0x989146     joy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load emotion labels\n",
    "emotion_df = pd.read_csv(EMOTION_FILE)\n",
    "print(f\"{len(emotion_df)} emotion labels\")\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file tells us which posts are for training and which are for testing. It's needed because the main JSON file contains all posts mixed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split information for 64171 posts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    47890\n",
       "test     16281\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train/test split information\n",
    "split_df = pd.read_csv(SPLIT_FILE)\n",
    "print(f\"Split information for {len(split_df)} posts\")\n",
    "split_df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm merging all three dataframes together using the `id` column. After merging, I have everything in one place.The text, the train/test split info, and the emotion labels are all under `df` variable. Test posts will have `NaN` in the emotion column since I don't know their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts: 64171\n",
      "Train posts: 47890\n",
      "Test posts: 16281\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  split emotion\n",
       "0  0x61fc95  We got the ranch, loaded our guns and sat up t...   test     NaN\n",
       "1  0x35663e  I bet there is an army of married couples who ...  train     joy\n",
       "2  0xc78afe                         This could only end badly.  train    fear\n",
       "3  0x90089c  My sister squeezed a lime in her milk when she...  train     joy\n",
       "4  0xaba820         and that got my head bobbing a little bit.   test     NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all data together\n",
    "df = posts_df.merge(split_df, on='id', how='left')\n",
    "df = df.merge(emotion_df, on='id', how='left')\n",
    "\n",
    "print(f\"Total posts: {len(df)}\")\n",
    "print(f\"Train posts: {len(df[df['split'] == 'train'])}\")\n",
    "print(f\"Test posts: {len(df[df['split'] == 'test'])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how emotions are distributed in the training data. This is important because the classes are very imbalanced. Joy is about 50% of the data while disgust is only 2%. This might affect which model that is going to be chosen or how it will be configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy         23797\n",
       "anger       10694\n",
       "surprise     6281\n",
       "sadness      3926\n",
       "fear         2009\n",
       "disgust      1183\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check emotion distribution in training data\n",
    "df[df['split'] == 'train']['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "\n",
    "For NLP models to work well, I need to clean the text first.\n",
    "I'm using `re` library (regular expressions) to remove URLs, handle @mentions and #hashtags, etc.\n",
    "I noticed that things like `!!!` or `???` or TYPING IN CAPS might actually be useful for emotion detection, so I'm converting them to text markers instead of just deleting them.\n",
    "\n",
    "Steps applyed:\n",
    "- Convert to lowercase\n",
    "- Remove URLs\n",
    "- Handle mentions (@user) and hashtags (#topic)\n",
    "- Convert punctuation patterns to text markers (preserve emotion signals)\n",
    "- Remove extra whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below cleans the text step by step using regex (`re.sub`).\n",
    "I first save the original to detect CAPS (before lowercasing), then remove URLs, convert @mentions and #hashtags to markers, and handle punctuation.\n",
    "\n",
    "The markers at the end (\"veryexcited\", \"shouting\") preserve the emotional signals from the original text.\n",
    "\n",
    "\"emptytext\" is a fallback for emoji-only posts that would otherwise become empty strings.\n",
    "<- wich actually caused error with my first submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESS. EXAMPLES\n",
      "Original: I HATE THIS!!! Why won't it work??\n",
      "Cleaned:  i hate this exclamation why won't it work question veryexcited questioning\n",
      "\n",
      "Original: Check out @user and #happiness\n",
      "Cleaned:  check out mention and hashtag happiness\n",
      "\n",
      "Original: Visit https://example.com for more info\n",
      "Cleaned:  visit for more info\n",
      "\n",
      "Original: üòò ‚òÇÔ∏è‚òÇÔ∏è\n",
      "Cleaned:  üòò ‚òÇÔ∏è‚òÇÔ∏è\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    original = text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Track emotion signals before removing them\n",
    "    has_exclamations = text.count('!') >= 2\n",
    "    has_questions = text.count('?') >= 2\n",
    "    has_caps = sum(1 for c in original if c.isupper()) > len(original) * 0.3\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # convert mentions and hashtags to markers\n",
    "    text = re.sub(r'@\\w+', ' mention ', text)\n",
    "    text = re.sub(r'#(\\w+)', r' hashtag \\1 ', text)\n",
    "    \n",
    "    # Convert punctuation patterns to text markers\n",
    "    text = re.sub(r'!+', ' exclamation ', text)\n",
    "    text = re.sub(r'\\?+', ' question ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Add emotion markers at the end\n",
    "    if has_exclamations:\n",
    "        text = text + \" veryexcited\"\n",
    "    if has_questions:\n",
    "        text = text + \" questioning\"\n",
    "    if has_caps:\n",
    "        text = text + \" shouting\"\n",
    "    \n",
    "    # Handle empty texts (e.g., emoji-only posts)\n",
    "    return text if text.strip() else \"emptytext\"\n",
    "\n",
    "# Test the preprocessing function\n",
    "test_texts = [\n",
    "    \"I HATE THIS!!! Why won't it work??\",\n",
    "    \"Check out @user and #happiness\",\n",
    "    \"Visit https://example.com for more info\",\n",
    "    \"üòò ‚òÇÔ∏è‚òÇÔ∏è\"  # Emoji-only post\n",
    "]\n",
    "\n",
    "#Preprocessing Examples\n",
    "print(\"PREPROCESS. EXAMPLES\")\n",
    "for t in test_texts:\n",
    "    print(f\"Original: {t}\")\n",
    "    print(f\"Cleaned:  {preprocess_text(t)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code bellow I apply the preprocessing function to all 64k posts. The `.apply()` method runs the function on each row.\n",
    "I'm also checking how many posts ended up as \"emptytext\" (the emoji-only ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty texts after preprocessing: 0\n",
      "\n",
      " SAMPLE: Preprocessed Texts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>we got the ranch, loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>i bet there is an army of married couples who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>this could only end badly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>my sister squeezed a lime in her milk when she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  We got the ranch, loaded our guns and sat up t...   \n",
       "1  I bet there is an army of married couples who ...   \n",
       "2                         This could only end badly.   \n",
       "3  My sister squeezed a lime in her milk when she...   \n",
       "4         and that got my head bobbing a little bit.   \n",
       "\n",
       "                                          clean_text  \n",
       "0  we got the ranch, loaded our guns and sat up t...  \n",
       "1  i bet there is an army of married couples who ...  \n",
       "2                         this could only end badly.  \n",
       "3  my sister squeezed a lime in her milk when she...  \n",
       "4         and that got my head bobbing a little bit.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing to all texts\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Check for empty texts\n",
    "empty_count = (df['clean_text'] == 'emptytext').sum()\n",
    "print(f\"Empty texts after preprocessing: {empty_count}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n SAMPLE: Preprocessed Texts\")\n",
    "df[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps\n",
    "\n",
    "This section focuses on creating features for classification model:\n",
    "- **TF-IDF features** - word-level n-grams (1-3 grams)\n",
    "- **Character n-grams** - capture writing style patterns (3-6 char n-grams)\n",
    "- **Manual features** - text statistics and emotion keyword counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Features\n",
    "\n",
    "- Text length and word count\n",
    "- Punctuation counts (exclamation, question marks)\n",
    "- Capital letter ratio\n",
    "- Emoji count\n",
    "- Emotion keyword matching (simple lexicon-based approach)\n",
    "\n",
    "Besides TF-IDF, I wanted to add some hand-crafted features that might help with emotion detection. Things like exclamation marks, CAPS, or words like \"hate\" and \"love\" are obvious emotion signals.\n",
    "\n",
    "The `+1` in `capital_ratio` is to avoid division by zero for empty texts.\n",
    "\n",
    "For emoji detection, I'm counting any character with unicode value above 127. It's not perfect (catches accented letters too), but good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 11 manual features\n",
      "Feature names: ['length', 'word_count', 'exclamation', 'question', 'capital_ratio', 'emoji_count', 'anger_kw', 'joy_kw', 'sad_kw', 'fear_kw', 'surprise_kw']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>question</th>\n",
       "      <th>capital_ratio</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>anger_kw</th>\n",
       "      <th>joy_kw</th>\n",
       "      <th>sad_kw</th>\n",
       "      <th>fear_kw</th>\n",
       "      <th>surprise_kw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76.742688</td>\n",
       "      <td>14.104860</td>\n",
       "      <td>0.212261</td>\n",
       "      <td>0.134640</td>\n",
       "      <td>0.051056</td>\n",
       "      <td>0.343816</td>\n",
       "      <td>0.058531</td>\n",
       "      <td>0.129560</td>\n",
       "      <td>0.047327</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>0.114974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43.342436</td>\n",
       "      <td>7.657896</td>\n",
       "      <td>0.658740</td>\n",
       "      <td>0.435182</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>1.072932</td>\n",
       "      <td>0.241939</td>\n",
       "      <td>0.356135</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.128258</td>\n",
       "      <td>0.332578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>105.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>703.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length    word_count   exclamation      question  capital_ratio  \\\n",
       "count  64171.000000  64171.000000  64171.000000  64171.000000   64171.000000   \n",
       "mean      76.742688     14.104860      0.212261      0.134640       0.051056   \n",
       "std       43.342436      7.657896      0.658740      0.435182       0.077572   \n",
       "min        2.000000      1.000000      0.000000      0.000000       0.000000   \n",
       "25%       43.000000      8.000000      0.000000      0.000000       0.019231   \n",
       "50%       72.000000     14.000000      0.000000      0.000000       0.032787   \n",
       "75%      105.000000     19.000000      0.000000      0.000000       0.055556   \n",
       "max      703.000000     89.000000     13.000000     12.000000       0.918033   \n",
       "\n",
       "        emoji_count      anger_kw        joy_kw        sad_kw       fear_kw  \\\n",
       "count  64171.000000  64171.000000  64171.000000  64171.000000  64171.000000   \n",
       "mean       0.343816      0.058531      0.129560      0.047327      0.015989   \n",
       "std        1.072932      0.241939      0.356135      0.217773      0.128258   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       73.000000      3.000000      3.000000      4.000000      2.000000   \n",
       "\n",
       "        surprise_kw  \n",
       "count  64171.000000  \n",
       "mean       0.114974  \n",
       "std        0.332578  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        3.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(row):\n",
    "\n",
    "    text = row['text'] if pd.notna(row['text']) else \"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic text statistics\n",
    "    features['length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    \n",
    "    # Punctuation features (emotion indicators)\n",
    "    features['exclamation'] = text.count('!')\n",
    "    features['question'] = text.count('?')\n",
    "    \n",
    "    # Capitalization (anger/excitement indicator)\n",
    "    features['capital_ratio'] = sum(1 for c in text if c.isupper()) / (len(text) + 1)\n",
    "    \n",
    "    # Emoji count (unicode characters above ASCII)\n",
    "    features['emoji_count'] = len([c for c in text if ord(c) > 127])\n",
    "    \n",
    "    # Emotion keyword matching (simple lexicon approach)\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Anger keywords\n",
    "    anger_words = ['hate', 'angry', 'mad', 'stupid', 'damn', 'hell', 'furious']\n",
    "    features['anger_kw'] = sum(1 for w in anger_words if w in text_lower)\n",
    "    \n",
    "    # Joy keywords\n",
    "    joy_words = ['happy', 'love', 'great', 'haha', 'lol', 'amazing', 'wonderful']\n",
    "    features['joy_kw'] = sum(1 for w in joy_words if w in text_lower)\n",
    "    \n",
    "    # Sadness keywords\n",
    "    sad_words = ['sad', 'sorry', 'cry', 'miss', 'depressed', 'alone']\n",
    "    features['sad_kw'] = sum(1 for w in sad_words if w in text_lower)\n",
    "    \n",
    "    # Fear keywords\n",
    "    fear_words = ['scared', 'afraid', 'fear', 'worried', 'panic', 'nervous']\n",
    "    features['fear_kw'] = sum(1 for w in fear_words if w in text_lower)\n",
    "    \n",
    "    # Surprise keywords\n",
    "    surprise_words = ['wow', 'omg', 'what', 'really', 'wtf', 'shocked']\n",
    "    features['surprise_kw'] = sum(1 for w in surprise_words if w in text_lower)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all rows\n",
    "feature_data = df.apply(extract_features, axis=1)\n",
    "feature_df = pd.DataFrame(feature_data.tolist())\n",
    "\n",
    "# Combine with main dataframe\n",
    "df = pd.concat([df, feature_df], axis=1)\n",
    "\n",
    "print(f\"Extracted {len(feature_df.columns)} manual features\")\n",
    "print(f\"Feature names: {list(feature_df.columns)}\")\n",
    "feature_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow is splitting the data based on the `split` column that has been loaded earlier. Using `.copy()` to avoid the \"SettingWithCopyWarning\" from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 47890\n",
      "Test samples: 16281\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test based on the split column\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization\n",
    "\n",
    "Creating of two types of TF-IDF features:\n",
    "1. **Word-level TF-IDF** with n-grams (1, 2, 3) - captures word meanings and short phrases\n",
    "2. **Character-level TF-IDF** with n-grams (3-6) - captures writing style and morphological patterns\n",
    "\n",
    "[sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "TF-IDF converts text into numbers. Each word/phrase gets a score based on how important it is in a document vs. the whole dataset.\n",
    "I'm using `ngram_range=(1,3)` to capture single words but also phrases like \"so happy\" or \"hate this so\". The `max_features=10000` limits vocabulary size so it doesn't get too big. `max_df=0.85` ignores words that appear in more than 85% of documents (like \"the\", \"is\") since they're not useful for classification.\n",
    "The `fit_transform` on training data learns the vocabulary, then I just `transform` the test data using the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word TF-IDF shape: (47890, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Word-level TF-IDF vectorizer\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=10000, # maximum vocabulary size\n",
    "    ngram_range=(1, 3), # Unigrams, bigrams, trigrams\n",
    "    min_df=2,   # minimum document frequency\n",
    "    max_df=0.85,    # Maximum document frequency (ignore very common words)\n",
    "    sublinear_tf=True   # sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "X_train_word = tfidf_word.fit_transform(train_df['clean_text'])\n",
    "X_test_word = tfidf_word.transform(test_df['clean_text'])\n",
    "\n",
    "print(f\"Word TF-IDF shape: {X_train_word.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same idea as before but at character level. Vectorizer looks at letter patterns instead of words. This can catch typos, repeated letters (\"nooo\"), and writing style. The `analyzer='char'` switches from word to character mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character TF-IDF shape: (47890, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Character-level TF-IDF vectorizer\n",
    "# This part should take approx. 40s\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    max_features=5000,  # maximum vocabulary size\n",
    "    ngram_range=(3, 6), # Character n-grams from 3 to 6\n",
    "    analyzer='char',    # character-level analysis\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data, transform both train and test\n",
    "X_train_char = tfidf_char.fit_transform(train_df['clean_text'])\n",
    "X_test_char = tfidf_char.transform(test_df['clean_text'])\n",
    "\n",
    "print(f\"Character TF-IDF shape: {X_train_char.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all features\n",
    "\n",
    "Getting the manual features into arrays. StandardScaler normalizes them so they're on the same scale as TF-IDF values, otherwise features like `length` (values in hundreds) would dominate over smaller features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual features shape: (47890, 11)\n"
     ]
    }
   ],
   "source": [
    "# Get manual features and scale them\n",
    "feature_cols = list(feature_df.columns)\n",
    "X_train_manual = train_df[feature_cols].values\n",
    "X_test_manual = test_df[feature_cols].values\n",
    "\n",
    "# Scale manual features\n",
    "scaler = StandardScaler()\n",
    "X_train_manual = scaler.fit_transform(X_train_manual)\n",
    "X_test_manual = scaler.transform(X_test_manual)\n",
    "\n",
    "print(f\"Manual features shape: {X_train_manual.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all three feature sets (word TF-IDF, character TF-IDF, and manual features) into one matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (47890, 15011)\n",
      "Test features shape: (16281, 15011)\n",
      "Total features: 15011\n"
     ]
    }
   ],
   "source": [
    "# Combine all features using scipy hstack (for sparse matrices)\n",
    "X_train = hstack([X_train_word, X_train_char, X_train_manual])\n",
    "X_test = hstack([X_test_word, X_test_char, X_test_manual])\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['emotion'].values\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps\n",
    "\n",
    "This section focuses on:\n",
    "- Training multiple classification models\n",
    "- Evaluating using cross-validation (F1 Macro score)\n",
    "- Selecting the best model\n",
    "- Generating predictions for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I try Logistic Regression model\n",
    "\n",
    "The `class_weight='balanced'` is here because teh data is imbalanced (50% joy, only 2% disgust). This tells the model to pay more attention to rare classes.\n",
    "`C=2.0` controls regularization. Higher value means less regularization. `n_jobs=-1` uses all CPU cores to speed things up.\n",
    "I'm using 3-fold cross-validation with F1 Macro score because that's how the competition evaluates submissions. This gives a more reliable estimate than just training once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1: Logistic Regression\n",
      "CV F1 Scores: [0.44767661 0.44679706 0.44104679]\n",
      "Mean CV F1 Score: 0.4452 (+/- 0.0029)\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Logistic Regression\n",
    "# This part takes approx. 2m 30s\n",
    "lr_model = LogisticRegression(\n",
    "    C=2.0,  # inverse of regularization strength\n",
    "    max_iter=1000,  # maximum iterations\n",
    "    class_weight='balanced',    #for handling class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation with F1 Macro (competition metric)\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train, y_train, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "print(\"Training Model 1: Logistic Regression\")\n",
    "print(f\"CV F1 Scores: {lr_cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model is Linear SVM\n",
    "\n",
    "Same idea with `class_weight='balanced'`. The `C` parameter is a bit lower (0.8) which means slightly more regularization. I increased `max_iter` to 2000 because SVM may need more iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2: Linear SVM\n",
      "CV F1 Scores: [0.43805779 0.43739666 0.43555725]\n",
      "Mean CV F1 Score: 0.4370 (+/- 0.0011)\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Linear SVM\n",
    "# This part takes about 3 m 10 s\n",
    "svm_model = LinearSVC(\n",
    "    C=0.8, # Regularization parameter\n",
    "    class_weight='balanced',\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_cv_scores = cross_val_score(svm_model, X_train, y_train, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "print(\"Model 2: Linear SVM\")\n",
    "print(f\"CV F1 Scores: {svm_cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {svm_cv_scores.mean():.4f} (+/- {svm_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third model is Random Forest. An ensemble of 300 decision trees.\n",
    "\n",
    "`max_depth=25` limits how deep each tree can grow. Without this, trees could overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3: Random Forest\n",
      "CV F1 Scores: [0.37630619 0.39009887 0.38924469]\n",
      "Mean CV F1 Score: 0.3852 (+/- 0.0063)\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Random Forest\n",
    "# This part took 2 m 15 s\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,   # Number of trees\n",
    "    max_depth=25,   # maximum tree depth\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "print(\"Model 3: Random Forest\")\n",
    "print(f\"CV F1 Scores: {rf_cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best model\n",
    "\n",
    "Comparing all three models by their CV F1 score and picking the best one. The winner will be used for final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression      : F1 = 0.4452\n",
      "Linear SVM               : F1 = 0.4370\n",
      "Random Forest            : F1 = 0.3852\n",
      "\n",
      "\n",
      " BEST MODEL: Logistic Regression\n",
      " Cross-validation F1 Score: 0.4452\n"
     ]
    }
   ],
   "source": [
    "# Compare all models\n",
    "models = {\n",
    "    'Logistic Regression': (lr_model, lr_cv_scores.mean()),\n",
    "    'Linear SVM': (svm_model, svm_cv_scores.mean()),\n",
    "    'Random Forest': (rf_model, rf_cv_scores.mean())\n",
    "}\n",
    "\n",
    "# Print comparison\n",
    "for name, (model, score) in models.items():\n",
    "    print(f\"{name:25s}: F1 = {score:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_name = max(models, key=lambda x: models[x][1])\n",
    "best_model, best_score = models[best_name]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\" BEST MODEL: {best_name}\")\n",
    "print(f\" Cross-validation F1 Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating predction\n",
    "\n",
    "Running the best model on test data. Checking the prediction distribution to make sure it looks reasonable (not predicting just one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16281 predictions\n",
      "\n",
      "Prediction Distribution:\n",
      "joy         5772\n",
      "anger       3730\n",
      "surprise    2478\n",
      "sadness     1906\n",
      "fear        1606\n",
      "disgust      789\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the best model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Generated {len(test_predictions)} predictions\")\n",
    "\n",
    "# Check prediction distribution\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "pred_dist = pd.Series(test_predictions).value_counts()\n",
    "print(pred_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating submission file\n",
    "\n",
    "Creating the submission file in the format required by the competition. Which is two columns: `id` and `emotion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Preview:\n",
      "          id   emotion\n",
      "0   0x61fc95      fear\n",
      "4   0xaba820      fear\n",
      "5   0x66e44d       joy\n",
      "6   0xc03cf5       joy\n",
      "8   0x02f65a     anger\n",
      "16  0x479407       joy\n",
      "20  0xe07a21      fear\n",
      "27  0x06d186       joy\n",
      "29  0xa9a658  surprise\n",
      "41  0x0a0102       joy\n",
      "\n",
      "Total rows: 16281\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "# Verify submission format\n",
    "print(\"Submission Preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nTotal rows: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to CSV before uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File saved to: C:/Users/Administrator/Documents/MUNI/2/NTNU_fall/Data Mining/DM2025Labs/DM2025-Lab2-Exercise/submission.csv\n",
      "Total predictions: 16281\n",
      "\n",
      "Best model: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Save submission to CSV\n",
    "submission_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\nFile saved to: {OUTPUT_FILE}\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(f\"\\nBest model: {best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "1. **Preprocessing:** Loaded JSON posts, merged with emotion labels and train/test split. Cleaned text by removing URLs, converting @mentions and #hashtags to markers. Preserved emotion signals (!!!, ???, CAPS) as text markers instead of removing them.\n",
    "\n",
    "2. **Feature Engineering:** Created TF-IDF features with word n-grams (1-3) and character n-grams (3-6). Extracted manual features like text length, punctuation counts, and emotion keyword matching. Combined everything into ~15,000 features.\n",
    "\n",
    "3. **Model:** Trained Logistic Regression, Linear SVM, and Random Forest. Evaluated with 3-fold cross-validation using F1 Macro. Logistic Regression performed best.\n",
    "\n",
    "### Explanation of the Model\n",
    "\n",
    "I compared three models using 3-fold cross-validation with F1 Macro (the competition metric):\n",
    "\n",
    "| Model | CV F1 Score |\n",
    "|-------|-------------|\n",
    "| Logistic Regression | 0.4452 |\n",
    "| Linear SVM | 0.4370 |\n",
    "| Random Forest | 0.3852 |\n",
    "\n",
    "Logistic Regression with `class_weight='balanced'` worked best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Things I Tried\n",
    "\n",
    "My first submission used basic TF-IDF with simple preprocessing (just removing URLs and punctuation). \n",
    "\n",
    "For the second submission, I made a few improvements:\n",
    "- Increased TF-IDF vocabulary size and added trigrams\n",
    "- Added emotion keyword matching (simple lists of words like \"hate\", \"love\", \"scared\")\n",
    "- Preserved emotion signals (!!!, ???, CAPS) as text markers instead of removing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Improvements\n",
    "\n",
    "- Pretrained embeddings (Word2Vec, GloVe) instead of TF-IDF\n",
    "- Use deep learning models (BERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
